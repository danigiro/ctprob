\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage[hidelinks]{hyperref}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 

\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%


\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{bm}

\usepackage{mathrsfs}
\usepackage{amssymb}
%\usepackage{mathpazo}
\usepackage[dvipsnames]{xcolor}
%\usepackage[no-weekday]{eukdate}
%\usepackage[bb=boondox]{mathalfa}
\usepackage{paralist}
\usepackage{natbib}
%\usepackage{url}
%\usepackage[textwidth=6.1in,textheight = 10in]{geometry}
%\usepackage{placeins}
\usepackage[hidelinks]{hyperref}
\usepackage{multirow, float}
\usepackage{makecell}
\usepackage{calc}
%\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{microtype}
%\usepackage{todonotes}

\usepackage[nodisplayskipstretch]{setspace}
\usepackage[bottom,hang,flushmargin]{footmisc}
\renewcommand{\footnotelayout}{\setstretch{1}}
\usepackage{adjustbox}

\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}

\usepackage{caption}
\DeclareCaptionStyle{italic}[justification=centering]
 {labelfont={bf},textfont={it},labelsep=colon}
\captionsetup[figure]{style=italic,singlelinecheck=true,skip=1ex}
\captionsetup[table]{style=italic,singlelinecheck=true,skip=1ex}
\usepackage{subcaption}

\usepackage{longtable}
\usepackage{booktabs}
\usepackage{array}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}m{#1}}

\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\alphavet}{\bm{\alpha}}
\newcommand{\betavet}{\bm{\beta}}
\newcommand{\epsvet}{\bm{\varepsilon}}
\newcommand{\etavet}{\bm{\eta}}
\newcommand{\lambdavet}{\bm{\lambda}}
\newcommand{\Unovet}{\bm{1}}
\newcommand{\avet}{\bm{a}}
\newcommand{\bvet}{\bm{b}}
\newcommand{\cvet}{\bm{c}}
\newcommand{\dvet}{\bm{d}}
\newcommand{\evet}{\bm{e}}
\newcommand{\pvet}{\bm{p}}
\newcommand{\fvet}{\bm{f}}
\newcommand{\tvet}{\bm{t}}
\newcommand{\uvet}{\bm{u}}
\newcommand{\vvet}{\bm{v}}
\newcommand{\wvet}{\bm{w}}
\newcommand{\xvet}{\bm{x}}
\newcommand{\yvet}{\bm{y}}
\newcommand{\zvet}{\bm{z}}
\newcommand{\Avet}{\bm{A}}
\newcommand{\Bvet}{\bm{B}}
\newcommand{\Cvet}{\bm{C}}
\newcommand{\Dvet}{\bm{D}}
\newcommand{\Evet}{\bm{E}}
\newcommand{\Fvet}{\bm{F}}
\newcommand{\Gvet}{\bm{G}}
\newcommand{\Hvet}{\bm{H}}
\newcommand{\Ivet}{\bm{I}}
\newcommand{\Jvet}{\bm{J}}
\newcommand{\Kvet}{\bm{K}}
\newcommand{\Lvet}{\bm{L}}
\newcommand{\Mvet}{\bm{M}}
\newcommand{\Nvet}{\bm{N}}
\newcommand{\Pvet}{\bm{P}}
\newcommand{\Qvet}{\bm{Q}}
\newcommand{\Rvet}{\bm{R}}
\newcommand{\Svet}{\bm{S}}
\newcommand{\Tvet}{\bm{T}}
\newcommand{\Uvet}{\bm{U}}
\newcommand{\Wvet}{\bm{W}}
\newcommand{\Xvet}{\bm{X}}
\newcommand{\Yvet}{\bm{Y}}
\newcommand{\Zvet}{\bm{Z}}
\newcommand{\Zerovet}{\bm{0}}
\newcommand{\Omegavet}{\bm{\Omega}}
\newcommand{\Sigmavet}{\bm{\Sigma}}
\newcommand{\phivet}{\bm{\phi}}

\definecolor{mybluehl}{HTML}{cbd3ff}

% theorem
\makeatletter
\def\@endtheorem{\endtrivlist}
\makeatother

%% tikz
%% Packages to draw hierarchies
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,positioning,shadows,trees}
\usetikzlibrary{matrix, decorations.pathreplacing, arrows, calc, fit, arrows.meta, decorations.pathmorphing, decorations.markings}

\tikzset{
  basic/.style  = {draw, text width=2cm, drop shadow, font=\sffamily, rectangle},
  root/.style   = {basic, rounded corners=2pt, thin, align=center,
                   fill=green!30},
  level 2/.style = {basic, rounded corners=6pt, thin,align=center, fill=green!60,
                   text width=4em},
  level 3/.style = {basic, thin, align=left, fill=pink!60, text width=1.5em}
}
\newcommand{\relation}[3]
{
	\draw (#3.south) -- +(0,-#1) -| ($ (#2.north) $)
}
\newcommand{\relationW}[2]
{
	\draw (#2.west) -| ($ (#1.north) $)
}
\newcommand{\relationE}[2]
{
	\draw (#2.east) -| ($ (#1.north) $)
}

\newcommand{\relationD}[3]
{
	\draw (#3.east) -- +(#1,0) |- (#2.west)
}

\pgfdeclareimage[height=0.85cm]{ngreen}{fig/boot/ngreen.pdf}
\pgfdeclareimage[height=0.85cm]{nblue}{fig/boot/nblue.pdf}
\pgfdeclareimage[height=0.85cm]{nred}{fig/boot/nred.pdf}
\pgfdeclareimage[height=0.85cm]{nblack}{fig/boot/nblack.pdf}

\pgfdeclareimage[height=0.4cm]{ngreen2}{fig/boot/ngreen.pdf}
\pgfdeclareimage[height=0.4cm]{nblue2}{fig/boot/nblue.pdf}
\pgfdeclareimage[height=0.4cm]{nred2}{fig/boot/nred.pdf}
\pgfdeclareimage[height=0.4cm]{nblack2}{fig/boot/nblack.pdf}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]

\makeatletter
\newcommand{\githuburl}{\begingroup%
\if1\blind
{
$\dots$
}\fi
\if0\blind
{
\url{https://github.com/danigiro/ctprob}.
}\fi
\endgroup}
\makeatother

\begin{document}
%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf \Large Cross-temporal Probabilistic Forecast Reconciliation}
  \author{\normalsize Daniele Girolimetto\thanks{Corresponding author. E-mail: \href{mailto:daniele.girolimetto@phd.unipd.it}{daniele.girolimetto@phd.unipd.it}}\\[-0.1cm]
\normalsize \textit{Department of Statistical Sciences, University of Padova}\\[-0.1cm]
\normalsize and\\[-0.1cm]
 George Athanasopoulos\\[-0.1cm]
\normalsize \textit{Department of Econometrics and Business Statistics, Monash University}\\[-0.1cm]
\normalsize and\\[-0.1cm]
\normalsize  Tommaso Di Fonzo\\[-0.1cm]
\normalsize \textit{Department of Statistical Sciences, University of Padova}\\[-0.1cm]
\normalsize and\\[-0.1cm]
\normalsize  Rob J Hyndman\\[-0.1cm]
\normalsize \textit{Department of Econometrics and Business Statistics, Monash University}}
\date{\normalsize\today\vspace{-0.5cm}}
  \maketitle
} \fi

%\if1\blind
%{
%  \bigskip
%  \bigskip
%  \bigskip
%  \begin{center}
%    {\LARGE\bf Cross-temporal Probabilistic Forecast Reconciliation}
%\end{center}
%  \medskip
%} \fi

\if1\blind
{
  \title{\bf Cross-temporal Probabilistic Forecast Reconciliation}
  \maketitle
  \bigskip
} \fi

%\bigskip
\begin{abstract}
\noindent Forecast reconciliation is a post-forecasting process that involves transforming a set of incoherent forecasts into coherent forecasts which satisfy a given set of linear constraints for a multivariate time series. In this paper we extend the current state-of-the-art cross-sectional probabilistic forecast reconciliation approach to encompass a cross-temporal framework, where temporal constraints are also applied. Our proposed methodology employs both parametric Gaussian and non-parametric bootstrap approaches to draw samples from an incoherent cross-temporal distribution.
	To improve the estimation of the forecast error covariance matrix, we propose using multi-step residuals, especially in the time dimension where the usual one-step residuals fail.
	To address high-dimensionality issues, we present four alternatives for the covariance matrix, where we exploit the two-fold nature (cross-sectional and temporal) of the cross-temporal structure, and introduce the idea of overlapping residuals. 
	We assess the effectiveness of the proposed cross-temporal reconciliation approaches through a simulation study that investigates their theoretical and empirical properties and two empirical forecasting experiments, using the Australian GDP and the Australian Tourism Demand datasets. For both applications, the optimal cross-temporal reconciliation approaches significantly outperform the incoherent base forecasts in terms of the Continuous Ranked Probability Score and the Energy Score. Overall, our study expands and unifies the notation for cross-sectional, temporal and cross-temporal reconciliation, thus extending and deepening the probabilistic cross-temporal framework. The results highlight the potential of the proposed cross-temporal forecast reconciliation methods in improving the accuracy of probabilistic forecasting models.
\end{abstract}

\noindent%
{\it Keywords:} Coherent, GDP, Linear constraints, Multivariate time series, Temporal aggregation, Tourism flows
\vfill

\newpage
\spacingset{1.8} % DON'T change the spacing!
\section{Introduction}

Forecast reconciliation is a post-forecasting process intended to improve the quality of forecasts for a system of linearly constrained multiple time series \citep{hyndman2011, panagiotelis2021}. There are many fields where forecast reconciliation is useful, such as when forecasting GDP and its components, electricity demand and power generation, demand in supply chains with product categories, tourist flows across geographic regions and travel purpose, and more. Moreover, effective decision-making depends on the support of accurate and coherent forecasts.

Classical reconciliation methods addressed the issue of incoherent forecasts in a cross-sectional hierarchy by forecasting only one level and using these to generate forecasts for the remaining series. The bottom-up approach \citep{dunn1976} starts by generating forecasts at the most disaggregate level and summing these to arrive at the desired forecasts for aggregate levels. On the other hand, the top-down approach \citep{gross1990} forecasts the most aggregated level and then disaggregates it to lower levels \citep{fliedner2001, athanasopoulos2009}. The middle-out method \citep{athanasopoulos2009} combines both approaches by selecting an intermediate level and applies top-down for lower levels and bottom-up for upper levels.

All of these approaches ignore useful information available at other levels \citep{pennings2017}. Consequently, in the last decade, hierarchical forecasting has significantly evolved to include modern least squares-based reconciliation techniques in the cross-sectional framework \citep{hyndman2011, wickramasuriya2019, panagiotelis2021}, later extended to temporal hierarchies \citep{athanasopoulos2017, nystrup2020}. Obtaining coherent forecasts across both the cross-sectional and temporal dimensions (known as \textit{cross-temporal coherence}) has been limited to sequential approaches that address each dimension separately \citep{kourentzes2019, yagli2019, punia2020, spiliotis2020}. Recently, \citet{difonzo2023} suggested a unified reconciliation step that takes into account both the cross-sectional and temporal dimensions, instead of dealing with them separately, utilizing the entire cross-temporal hierarchy.

However, these cross-temporal works focus on point forecasting, and do not consider distributional or probabilistic forecasts \citep{gneiting2014}. In the cross-sectional and temporal frameworks, there have been some developments towards probabilistic forecasting including  \cite{bentaieb2017}, \cite{panamtash2018}, \cite{jeon2019}, \cite{yang2020}, \cite{yagli2020}, 
\cite{bentaieb2021}, \cite{corani2021}, \cite{corani2022}, \cite{zambon2022} and \cite{wickramasuriya2021b}. \cite{panagiotelis2023} made a significant contribution by formalizing cross-sectional probabilistic reconciliation using the geometric framework for point forecast reconciliation of \cite{panagiotelis2021}. They show how a reconciled forecast can be constructed from an arbitrary base forecast when its density is available and when only a sample can be drawn. They also show that in the case of elliptical distributions, the correct predictive distribution can be recovered via linear reconciliation, regardless of the base forecast location and scale parameters, and derive conditions for this to hold in the special case of reconciliation via projection. 

In this paper, we extend cross-sectional probabilistic reconciliation to the cross-temporal case, working on issues related to the two-fold nature of this framework. First, we revise and develop the notation proposed by \cite{difonzo2023} to generalize the work of \cite{panagiotelis2023}. This allows us to move from cross-temporal point reconciliation to a probabilistic setting through the generalization of definitions and theorems well-established in the cross-sectional framework. Second, we propose effective and practical solutions to draw a sample from the base forecast distribution according to either a parametric approach that assumes Gaussianity or a non-parametric approach that bootstraps the base model residuals. 
Third, we propose some solutions to specific problems that arise when combining the cross-sectional and temporal dimensions. We propose using multi-step residuals to estimate the relationships between different forecast horizons when we deal with temporal levels, since one-step residuals are not suitable for this purpose. To solve high-dimensionality issues we introduce the idea of overlapping residuals and consider alternative forms for constructing the covariance matrix. Fourth, we propose new shrinkage procedures for reconciliation that aim to identify a feasible cross-temporal structure. The methodological contributions described in this paper 
are implemented in the \texttt{FoReco} package \citep{foreco2023} for R \citep{rcoreteam2022}. Furthermore, the Appendix contains complementary materials on methodological and practical issues, and supplementary tables and graphs related to the empirical applications.

The remainder of the paper is structured as follows. In \autoref{sec:not}, we provide a unified notation for the cross-sectional, temporal and cross-temporal point reconciliation. We generalize the cross-sectional definitions and theorems developed by \cite{panagiotelis2023} in \autoref{sec:prob}, and propose both a parametric Gaussian and a non-parametric bootstrap approach to draw a sample from the base forecast distribution. In \autoref{sec:shrtech}, we analyze the structure of the cross-temporal covariance matrix, proposing four alternative forms, and propose shrinkage approaches for reconciliation. In addition, we explore cross-temporal residuals (overlapping and multi-step) looking at their advantages and limitations. %A simulation study is performed in \autoref{sec:mcsim}, to better understand the properties of the methodology. 
Two empirical applications using the Australian GDP and the Australian Tourism Demand datasets are considered in Sections \ref{sec:ausgdp} and \ref{sec:vn525}, respectively\footnote{A complete set of results is available at the GitHub repository \githuburl}. Finally, \autoref{sec:conclusion} presents conclusions and a future research agenda on this and other related topics.


\section{Notation and definitions}\label{sec:not}

Let $\yvet_t = [y_{1,t},\dots,y_{i,t},\dots,y_{n,t}]'$ be an $n$-variate linearly constrained time series observed at the most temporally disaggregated level, with a seasonality of period $m$ (e.g., $m = 12$ for monthly data, $m = 4$ for quarterly data, $m = 24$ for hourly data). Suppose that the constraints are expressed by linear equations such that \citep{difonzo2023}
\begin{equation}
	\label{eq:cs_con}
	\Cvet_{cs}\yvet_t = \Zerovet_{(n_a \times 1)}, \qquad t = 1, \;\dots, \;T,
\end{equation}
where $\Cvet_{cs}$ is the $(n_a \times n)$ zero constraints cross-sectional matrix, that can be seen as the coefficient matrix of a linear system with $n_a$ equations and $n$ variables.

\input{./fig/hier.tex}

An example is a hierarchical time series where series at upper levels can be expressed by appropriately summing part or all of the series at the bottom level. \autoref{fig:hierS}(a) shows the two-level hierarchical structure for three linearly constrained time series such that $y_{T,t} = y_{X,t} + y_{Y,t}$, $\forall t = 1,...,T$. Now let $\yvet_t = \left[\uvet_t' \quad \bvet_t'\right]'$, where $\uvet_t = [y_{1,t}, \dots, y_{n_a,t}]'$ is the $n_a$-vector of upper levels time series and $\bvet_t = \left[y_{(n_a+1),t}\quad \dots \quad y_{n,t}\right]'$ is the $n_b$-vector of bottom level time series with $n = n_a+n_b$. The upper and lower level time series are connected by the cross-sectional aggregation matrix $\Avet_{cs}$ such that $\uvet_t = \Avet_{cs}\bvet_t$. Following \cite{giro2022}, we can always construct a zero-constraints cross-sectional matrix from the aggregation matrix, $\Cvet_{cs}=\left[\Ivet_{n_a} \quad {-\Avet_{cs}}\right]$ . Finally, the cross-sectional structural matrix is given by $\Svet_{cs} = \left[\begin{array}{c}
	\Avet_{cs}\\[-0.25cm]
	\Ivet_{n_b}
\end{array}\right]$, providing the structural representation \citep{hyndman2011} $\yvet_t = \Svet_{cs} \bvet_t$. Considering the hierarchical example in \autoref{fig:hierS}(a), we have $\Avet_{cs} = \left[ 1 \quad 1 \right]$, $\Cvet_{cs} = \left[ 1 \quad -1\quad -1 \right]$. In general there is no reason for $\uvet_t$ to be restricted to simple sums of $\bvet_t$; therefore $\Avet_{cs} \in \mathbb{R}^{n_a\times n_b}$ may contain any real values, and not only 0s and 1s.

Considering now the temporal framework, we denote as $\mathcal{K} = \{ k_p , k_{p-1}, \dots, k_2, k_1 \}$ the set of $p$ factors of $m$, in descending order, where $k_1= 1$ and $k_p= m$ \citep{athanasopoulos2017}. Given a factor $k$ of $m$, and assuming that $T = N m$ (where $N$ is the length of the most temporally aggregated version of the series), we can construct a temporally aggregated version of the time series of a single variable $\{\yvet_{i,t}\}_{t = 1, \dots, T}$, through the non-overlapping sums of its $k$ successive values, which has a seasonal period equal to $M_k= \displaystyle\frac{m}{k}$: $x_{i,j}^{[k]} = \displaystyle\sum_{t=(j-1)k+1}^{jk} y_{i,t}$, where $j = 1,\dots, N_k$, $i = 1,\dots,n$, $N_k = \displaystyle\frac{T}{k}$ and $x_{i,j}^{[1]}=y_{i,t}$. Define $\tau$ as the observation index of the most aggregate level $k_p$. For a fixed temporal aggregation order $k \in \mathcal{K}$, we stack the observations in the column vector $\xvet_{i,\tau}^{[k]} = \left[x_{i,M_k(\tau-1)+1}^{[k]} \quad x_{i,M_k(\tau-1)+2}^{[k]} \quad \dots \quad x_{i,M_k\tau}^{[k]}\right]',$ and obtain the vector for all the temporal aggregation orders $\xvet_{i,\tau} = \left[x_{i,\tau}^{[k_p]} \quad \xvet_{i,\tau}^{[k_{p-1}]\prime} \quad \dots \quad \xvet_{i,\tau}^{[1]\prime} \right]'$, $\tau = 1,\dots,N$. The structural representation of the temporal hierarchy \citep{athanasopoulos2017} is then $\xvet_{i,\tau} = \Svet_{te}\xvet_{i,\tau}^{[1]}$, where $\Svet_{te} = \left[\begin{array}{c}
	\Avet_{te} \\[-0.25cm]
	\Ivet_{m}
\end{array}\right]$ is the $[(m+k^\ast) \times m]$ temporal structural matrix, $\Avet_{te} = \left[\Unovet_{k_p} \quad \Ivet_{\frac{m}{k_{p-1}}} \otimes \Unovet_{k_{p-1}} \quad \dots \quad \Ivet_{\frac{m}{k_{2}}}  \otimes \Unovet_{k_2} \right]'$
%$$
%	\Avet_{te} = \begin{bmatrix}
%		\multicolumn{3}{c}{\Unovet_{k_p}'}                       \\
%		\Ivet_{\frac{m}{k_{p-1}}} & \otimes & \Unovet_{k_{p-1}}' \\
%		                          & \vdots  &                    \\
%		\Ivet_{\frac{m}{k_{2}}}   & \otimes & \Unovet_{k_2}'
%	\end{bmatrix}
%$$
is the $(k^\ast \times m)$ temporal aggregation matrix with $k^\ast = \displaystyle\sum_{k \in \mathcal{K}\setminus\{k_1\}} M_k$, and $\otimes$ is the Kronecker product. For each series $x_{i,\tau}$, $i = 1,\dots,n$, we have also the zero-constrained representation
\begin{equation}
	\label{eq:te_con}
	\Cvet_{te}\xvet_{i,\tau} = \Zerovet_{[k^\ast \times (m+k^\ast)]}, \qquad \tau = 1,\dots,N, \qquad i = 1,\dots, n
\end{equation}
where $\Cvet_{te} = [\Ivet_{k^\ast} ~~ {-\Avet_{te}}]$ is the $[k^\ast \times (m+k^\ast)]$ zero constraints temporal matrix. \autoref{fig:hierS}(b) shows the hierarchical representation of a quarterly time series, for which $m = 4$, $\mathcal{K} = \{4,2,1\}$.% and
%$$
%	\Avet_{te} = \begin{bmatrix}
%		1 & 1 & 1 & 1 \\
%		1 & 1 & 0 & 0 \\
%		0 & 0 & 1 & 1 \\
%	\end{bmatrix}, \quad \Cvet_{te} = \begin{bmatrix}
%		1 & 0 & 0 & -1 & -1 & -1 & -1 \\
%		0 & 1 & 0 & -1 & -1 & 0  & 0  \\
%		0 & 0 & 1 & 0  & 0  & -1 & -1 \\
%	\end{bmatrix} \quad \mathrm{and} \quad \Svet_{te} = \begin{bmatrix}
%		\Avet_{te} \\
%		\Ivet_4
%	\end{bmatrix}.
%$$
When we temporally aggregate each series, the cross-sectional constraints for the most temporally disaggregated series \eqref{eq:cs_con} hold for all the temporal aggregation orders such that $\Cvet_{cs}\xvet^{[k]}_j = \Zerovet_{(n_a \times 1)}$, for $k \in \mathcal{K}$ and $j = 1, \dots, N_k$, where $\xvet_j^{[k]} = \left[\uvet_j^{[k]\prime}\quad \bvet_j^{[k]\prime}\right]'$ with $\uvet^{[k]}_j = \left[ x^{[k]}_{1,\;j}\quad \dots\quad x^{[k]}_{n_a,\;j}\right]'$ is the $n_a$-vector of upper time series and $\bvet^{[k]}_j = \left[x^{[k]}_{(n_a+1),\;j}\quad\dots\quad x^{[k]}_{n,\;j}\right]'$ is the $n_b$-vector of bottom time series in the temporal hierarchy.

To include both cross-sectional and temporal constraints at the same time in a unified framework, we stack the series into a $[n \times (m+k^\ast)]$ matrix $\Xvet_\tau$, whose rows and columns represent, respectively, the cross-sectional and the temporal dimension: $\Xvet_\tau = \left[\xvet_{1,\tau}'\quad \dots \quad \xvet_{n,\tau}\right]' = \left[\Xvet_{\tau}^{[k_p]} \quad \dots \quad \Xvet_{\tau}^{[k_1]}\right]$ with $\Xvet_{\tau}^{[k_p]} = \left[\Uvet_{\tau}^{[k]'} \quad
		\Bvet_{\tau}^{[k]'}\right]'$,
%\begin{equation}
%	\label{eq:Xtau}
%	\Xvet_\tau = \begin{bmatrix}
%		\xvet_{1,\tau}' \\
%		\vdots          \\
%		\xvet_{n,\tau}'
%	\end{bmatrix} = \begin{bmatrix}
%		\Xvet_{\tau}^{[k_p]} & \dots & \Xvet_{\tau}^{[k_1]} \\ \end{bmatrix}
%	\quad \text{with} \quad \Xvet_{\tau}^{[k]} = \begin{bmatrix}
%		\Uvet_{\tau}^{[k]} \\
%		\Bvet_{\tau}^{[k]},
%	\end{bmatrix},
%\end{equation}
where for any fixed $k$,
$\Uvet_{\tau}^{[k]}$ is the ($n_a\times N_k$) matrix grouping the upper time series, $\Bvet_{\tau}^{[k]}$ is the ($n_b\times N_k$) matrix grouping the bottom time series. Further, $\Cvet_{cs}\Xvet_\tau = \Zerovet_{\left[n_a \times (m+k^\ast)\right]}$ and $\Cvet_{te}\Xvet_\tau' = \Zerovet_{(k^\ast \times n)} $. We can consider the cross-temporal framework as a generalization of the cross-sectional and temporal frameworks, that simultaneously takes into account both types of constraints. The cross-sectional reconciliation approach proposed by \cite{hyndman2011} can be obtained by assuming $m = 1$, while the temporal one \citep{athanasopoulos2017} is obtained when $n = 1$ (with $n_a = 0$ and $n_b = 1$).

%\input{./fig/Stilde_mat.tex}

\cite{difonzo2023} show that the cross-temporal constraints working on the complete set of observations corresponding to time period $\tau$ can be expressed in a zero-constrained representation through the full rank $\left[(n_am+nk^\ast)\times n(m+k^\ast)\right]$ zero constraints cross-temporal matrix $\Cvet_{ct}$ such that
\begin{equation}
	\label{eq:Cct}
	\Cvet_{ct} = \begin{bmatrix}
		\Cvet_\ast \\[-0.25cm]
		\Ivet_n \otimes \Cvet_{te}
	\end{bmatrix} \quad \Longrightarrow \quad
	\Cvet_{ct} \xvet_{\tau} = \Zerovet_{[(n_am+nk^\ast)\times1]} \quad \mathrm{for} \quad \tau = 1,\dots,N,
\end{equation}
where $\xvet_{\tau} = \mathrm{vec}(\Xvet_{\tau}') = [\xvet_{1, \tau}',~ 	\dots, ~ \xvet_{n, \tau}']'$, $\Cvet_\ast = [\Zerovet_{(n_a m\times nk^\ast)} ~~ \Ivet_m \otimes \Cvet_{cs}]\Pvet'$, and $\Pvet$ is the commutation matrix \citep[][p. 54]{magnus2019} such that $\Pvet \mathrm{vec}(\Yvet_{\tau}) = \mathrm{vec}(\Yvet_{\tau}')$. A structural representation can be considered as well: $\xvet_\tau = \Svet_{ct}\bvet^{[1]}_\tau = s(\bvet_{\tau}^{[1]})$, where
\begin{equation}
	\label{eq:Sct}
	\Svet_{ct} = \Svet_{cs} \otimes \Svet_{te}
	%\vspace{-0.25cm}
\end{equation}
is the $\left[n(k^\ast+m)\times n_b m\right]$ cross-temporal summation matrix, $s: \mathbb{R}^{n_b m} \rightarrow \mathbb{R}^{n(m+k^\ast)}$ is the operator describing the pre-multiplication by $\Svet_{ct}$, and $\bvet^{[1]}_\tau = \mathrm{vec}(\Bvet^{[1]\prime}_{\tau})$. 

In agreement with \cite{panagiotelis2021}, $\xvet_{\tau}$ lies in an $(n_b m)$-dimensional subspace $\mathfrak{s}_{ct}$ of $\mathbb{R}^{n(k^\ast+m)}$, which we refer to as the \textit{cross-temporal coherent subspace}, spanned by the columns of $\Svet_{ct}$. %In \autoref{fig:Stilde}, we have represented $\Svet_{ct}$ for 3 linearly constrained quarterly time series, shown in \autoref{fig:hierS}.

\subsection{Optimal point forecast reconciliation}\label{ssec:oct}

Let $\widehat{\xvet}_{h} = \mathrm{vec}(\widehat{\Xvet}_{h}')$, $h = 1, \dots, H$, be the $h$-step ahead base forecasts (however obtained) with error covariance matrix given by $\bm{W}_h = \text{Var}(\widehat{\xvet}_h - \xvet)$, where $H$ is the forecast horizon for the most temporally aggregated time series. Denote
$$
	\widehat{\Xvet}_{h} = \begin{bmatrix}
		\widehat{\xvet}_{1,h} \\[-0.25cm]
		\vdots                \\[-0.25cm]
		\widehat{\xvet}_{n,h}
	\end{bmatrix} =\begin{bmatrix}
		\widehat{\Uvet}_{h}^{[m]} & \dots & \widehat{\Uvet}_{h}^{[k]} & \dots & \widehat{\Uvet}_{h}^{[1]} \\[0.25cm]
		\widehat{\Bvet}_{h}^{[m]} & \dots & \widehat{\Bvet}_{h}^{[k]} & \dots & \widehat{\Bvet}_{h}^{[1]} \\\end{bmatrix},
$$
where $\widehat{\Uvet}_{h}^{[k]}$ is the ($n_a\times M_k$) matrix grouping the upper time series and $\widehat{\Bvet}_{h}^{[k]}$ is the ($n_b\times M_k$) matrix grouping the bottom time series for a given temporal aggregation order $k$. The matrix $\widehat{\Xvet}_{h}$, %organized as ${\Xvet}_{\tau}$, 
contains incoherent forecasts, such as $\Cvet_{ct} \widehat{\xvet}_{h} \neq \Zerovet_{[(n_am+nk^\ast)\times1]}$
with $h = 1, \dots, H$ and $\widehat{\xvet}_{h} = \mathrm{vec}(\widehat{\Xvet}_{h}')$. In this framework, the definition for forecast reconciliation in the cross-sectional framework given by \cite{panagiotelis2021} can be generalized as follows.

\begin{definition}
	Forecast reconciliation aims to adjust the base forecast $\widehat{\xvet}_{h}$ by finding a mapping $\psi: \mathbb{R}^{n(m+k^\ast)} \rightarrow \mathfrak{s}$ such that $\widetilde{\xvet}_{h} = \psi\left(\widehat{\xvet}_{h}\right)$, where $\widetilde{\xvet}_{h} \in \mathfrak{s}$ is the vector of the reconciled forecasts.
\end{definition}

For a given forecast horizon $h = 1,\dots, H$, the mapping $\psi$ may be defined as a projection onto $\mathfrak{s}$ given by \citep{panagiotelis2021, difonzo2023}
\begin{equation}
	\label{eq:Mvet}
	\widetilde{\xvet}_{h} = \psi\left(\widehat{\xvet}_h\right) = \Mvet \widehat{\xvet}_h,
\end{equation}
where $\Mvet = \Ivet_{n(m+ k^\ast)} - \Omegavet_{ct}\Cvet'_{ct}\left(\Cvet_{ct}\Omegavet_{ct}\Cvet'_{ct}\right)^{-1}\Cvet_{ct}$, for a positive definite matrix $\Omegavet_{ct}$, and $\widetilde{\xvet}_{h} = \mathrm{vec}(\widetilde{\Xvet}'_{h})$.
\citet{wickramasuriya2019} showed that the minimum variance linear unbiased reconciled forecasts, satisfying the unbiased condition $\text{E}(\widetilde{\xvet}_h -\xvet_h) = 0$, has solution \eqref{eq:Mvet} when $\Omegavet_{ct} = \text{Var}(\widetilde{\xvet}_h -\xvet_h)$.

Alternatively, the cross-temporal reconciled forecasts $\widetilde{\Xvet}_{h}$ may be found according to the structural approach proposed by \cite{hyndman2011} for the cross-sectional framework, yielding $\widetilde{\xvet}_h = \Svet_{ct}\Gvet \widehat{\xvet}_h$ for some matrix $\Gvet$. \citet{wickramasuriya2019} showed that this leads to a solution equivalent to the cross-temporally reconciled forecasts in \eqref{eq:Mvet}, given by
\begin{equation}\label{eq:SGy}
	\widetilde{\xvet}_{h} = \psi\left(\widehat{\xvet}_h \right) = \left(s \circ g \right)\left(\widehat{\xvet}_h\right)=\Svet_{ct}\Gvet \widehat{\xvet}_{h},
\end{equation}
where $\Gvet = (\Svet_{ct}' \Omegavet_{ct}^{-1}\Svet_{ct})^{-1} \Svet_{ct}'\Omegavet_{ct}^{-1}$,~ and $\Mvet = \Svet_{ct} \Gvet$. In this case, $\psi$ is the composition of two transformations, say $s \circ g$, where $g: \mathbb{R}^{n(m+k^\ast)} \rightarrow \mathbb{R}^{n_b m}$ is a continuous function. In Appendix A we report some cross-sectional, temporal and cross-temporal approximations for the covariance matrix to be used in \eqref{eq:Mvet} and \eqref{eq:SGy}.

\subsection{Cross-temporal bottom-up forecast reconciliation}\label{ssec:ctbu}

The classic bottom-up approach \citep{dunn1976, dangerfield1992} simply consists in summing-up the base forecasts of the most disaggregated level in the hierarchy to obtain forecasts of the upper-level series. To reduce the computational cost involved in optimal cross-temporal reconciliation, we may be interested in applying a reconciliation along only one dimension (cross-sectional or temporal) and reconstructing the cross-temporal structure using a partly bottom-up approach \citep{difonzo2022b, difonzo2023a, sanguri2022}.

\begin{figure}[!t]
	\centering

	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\caption{$\widetilde{\Xvet}$ with ct$(rec_{cs}, bu_{te})$}
		\resizebox{\linewidth}{!}{
			\begin{tikzpicture}[>=latex, line width=1pt,
				Matrix/.style={
				matrix of nodes,
				font=\large,
				align=center,
				text width = 1.5cm,
				text height = 0.65cm,
				column sep=2pt,
				row sep=7pt,
				nodes in empty cells,
				left delimiter={[},
						right delimiter={]},
						ampersand replacement=\&
						}]
				\matrix[Matrix] (Mt){ % Matrix contents
				$\widetilde{\Uvet}_{te(bu)}^{[m]}$ \& \dots \& $\widetilde{\Uvet}_{te(bu)}^{[k_2]}$ \& $\widetilde{\Uvet}_{cs(rec)}^{[1]}$ \\
				$\widetilde{\Bvet}^{[m]}_{te(bu)}$ \& \dots \& $\widetilde{\Bvet}^{[k_2]}_{te(bu)}$ \& $\widetilde{\Bvet}^{[1]}_{cs(rec)}$ \\
				};
				\draw[<-, opacity = 0] (Mt.north east)++(0.4,0) coordinate (temp) -- (temp |- Mt.south) node [midway,label={[label distance=0.1cm,rotate=-90, xshift = 1.5mm, font=\footnotesize]Cross-sectional}]{};
				\draw[<-] (Mt.south west)++(0,-0.15) coordinate (temp) -- (temp -| Mt.east) node [midway,label={[label distance=0cm,xshift = 1.5mm, font=\footnotesize]below:Temporal}]{};
				\node[opacity=0.2,
					rounded corners,
					inner sep=0pt, fill = blue, fit=(Mt-1-4)(Mt-2-4)](Bt){};
				\node[opacity=0.2,
					rounded corners,
					inner sep=0pt, fill = red, fit=(Mt-1-1)(Mt-2-3)](At){};
			\end{tikzpicture}}
		\label{fig:tebu}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\caption{$\widetilde{\Xvet}$ with ct$(rec_{te}, bu_{cs})$}
		\resizebox{\linewidth}{!}{
			\begin{tikzpicture}[>=latex, line width=1pt,
				Matrix/.style={
				matrix of nodes,
				font=\large,
				align=center,
				text width = 1.5cm,
				text height = 0.65cm,
				column sep=2pt,
				row sep=7pt,
				nodes in empty cells,
				left delimiter={[},
						right delimiter={]},
						ampersand replacement=\&
						}]
				\matrix[Matrix] (Mcs){ % Matrix contents
				$\widetilde{\Uvet}_{cs(bu)}^{[m]}$ \& \dots \& $\widetilde{\Uvet}_{cs(bu)}^{[k_2]}$ \& $\widetilde{\Uvet}_{cs(bu)}^{[1]}$ \\
				$\widetilde{\Bvet}^{[m]}_{te(rec)}$ \& \dots \& $\widetilde{\Bvet}^{[k_2]}_{te(rec)}$ \& $\widetilde{\Bvet}^{[1]}_{te(rec)}$ \\
				};
				\draw[<-] (Mcs.north east)++(0.4,0) coordinate (temp) -- (temp |- Mcs.south) node [midway,label={[label distance=0.1cm,rotate=-90, xshift = 1.5mm, font=\footnotesize]Cross-sectional}]{};
				\draw[<-, opacity = 0] (Mcs.south west)++(0,-0.15) coordinate (temp) -- (temp -| Mcs.east) node [midway,label={[label distance=0cm,xshift = 1.5mm, font=\footnotesize]below:Temporal}]{};
				\node[opacity=0.2,
					rounded corners,
					inner sep=0pt, fill = blue, fit=(Mcs-2-1)(Mcs-2-4)](Bcs){};
				\node[opacity=0.2,
					rounded corners,
					inner sep=0pt, fill = red, fit=(Mcs-1-1)(Mcs-1-4)](Acs){};
			\end{tikzpicture}}
		\label{fig:csbu}
	\end{subfigure}
	\vspace{-0.25cm}
	\caption{A visual representation of partly bottom up starting from \eqref{fig:tebu} cross-sectionally reconciled forecasts for the temporal order 1 ($\widetilde{\Uvet}^{[1]}$ and $\widetilde{\Bvet}^{[1]}$) followed by temporal bottom-up, and \eqref{fig:csbu} temporally reconciled forecasts of the cross-sectional bottom time series $(\widetilde{\Bvet}^{[k]}, \, k\in \mathcal{K})$ followed by cross-sectional bottom-up. %The \colorbox{mybluehl}{blue} background indicates generating reconciled forecasts along one dimension, while the \colorbox{pink}{pink} background indicates the forecasts obtained using bottom-up along the other.
	}
	\label{fig:bigBU}
	%\vspace*{-0.75\baselineskip}
\end{figure}

\autoref{fig:bigBU} provides a visual representation of partly bottom-up in a two-step cross-temporal reconciliation approach. On the left (\autoref{fig:tebu}), we first compute the cross-sectionally reconciled forecasts at the highest frequency ($k = 1$), and then apply temporal bottom-up to obtain coherent cross-temporal forecasts. On the right (\autoref{fig:csbu}), we first compute temporally reconciled forecasts for the most disaggregated cross-sectional level, and then apply the cross-sectional bottom-up. We denote these two-step reconciliation approaches, respectively, as ct$(rec_{te},bu_{cs})$, and ct$(rec_{cs},bu_{te})$, where ‘$rec_{te}$’ and ‘$rec_{cs}$’ denote a forecast reconciliation approach in the temporal and cross-sectional dimensions and, ‘$bu_{cs}$’ and ‘$bu_{te}$’ denote using bottom-up in the cross-sectional and temporal dimensions, respectively. It is worth noting that the simple cross-temporal bottom-up approach corresponds to $\mathrm{ct}(bu_{cs}, bu_{te})=\mathrm{ct}(bu_{te}, bu_{cs})=\mathrm{ct}(bu)$.

\section{Probabilistic forecast reconciliation}\label{sec:prob}

To introduce the idea of coherence and probabilistic forecast reconciliation, we adapt the notations and the formal definitions introduced in \cite{wickramasuriya2021b} and \cite{panagiotelis2023} for the cross-sectional probabilistic case. These definitions can also be generalized to the cross-temporal framework by following the approach developed by \cite{corani2022} for count data. However, in this paper we only focus on the continuous case.

Our aim is to extend these definitions to \textit{cross-temporal coherent probabilistic forecasts} and \textit{cross-temporal probabilistic forecast reconciliation}. Let $(\mathbb{R}^{n_b m}, \mathcal{F}_{\mathbb{R}^{n_b m}}, \nu)$ be a probability space for the bottom time series $\bvet_{\tau}^{[1]}$, where $\mathcal{F}_{\mathbb{R}^{n_b m}}$ is the Borel $\sigma$-algebra on $\mathbb{R}^{n_b m}$. Then a $\sigma$-algebra $\mathcal{F}_{\mathfrak{s}}$ can be constructed from the collection of sets $s(\mathcal{B})$ for all $\mathcal{B} \in \mathcal{F}_{\mathbb{R}^{n_b m}}$.
\begin{definition}[Cross-temporal coherent probabilistic forecasts]
	Given the probability space $(\mathbb{R}^{n_b m}, \mathcal{F}_{\mathbb{R}^{n_b m}}, \nu)$, we define the coherent probability space as the triple $(\mathfrak{s}, \mathcal{F}_{\mathfrak{s}}, \breve{\nu})$ satisfying the following property:
	$\breve{\nu}(s(\mathcal{B}))=\nu(\mathcal{B})$, $\forall \mathcal{B} \in \mathcal{F}_{\mathbb{R}^{n_b m}}$.
\end{definition}
Let $(\mathbb{R}^{n(m+k^\ast)}, \mathcal{F}_{\mathbb{R}^{n(m+k^\ast)}}, \hat{\nu})$ be a probability space referring to the incoherent probabilistic forecast ($\widehat{\xvet}_{h}$) for all the $n$ series in the system at any temporal aggregation order $k \in \mathcal{K}$.
\begin{definition}[Cross-temporal probabilistic forecast reconciliation]\label{def:pfr}
	The reconciled probability measure of $\hat{\nu}$ with respect to $\psi$ is a probability measure $\tilde{\nu}$ on $\mathfrak{s}$ with $\sigma$-algebra $\mathcal{F}_{\mathfrak{s}}$ satisfying
	\begin{equation}\label{eq:pfr}
		\tilde{\nu}(\mathcal{A})=\hat{\nu}(\psi^{-1}(\mathcal{A})), \quad \forall \mathcal{A} \in \mathcal{F}_{\mathfrak{s}},
	\end{equation}
	where $\psi^{-1}(\mathcal{A})=\{x \in \mathbb{R}^{n(m+k^\ast)}: \psi(x) \in \mathcal{A}\}$ denotes the pre-image of $\mathcal{A}$.
\end{definition}
The map $\psi$ may be obtained as the composition $s \circ g$, as for the cross-temporal point reconciliation \eqref{eq:SGy}. 

\begin{theorem}[Cross-temporal reconciled samples] \label{thm:rs}
	Suppose that $(\widehat{\xvet}_1, \dots, \widehat{\xvet}_L)$ is a sample drawn from a (cross-temporal) incoherent probability measure $\widehat{\nu}$. Then $(\widetilde{\xvet}_1, \dots, \widetilde{\xvet}_L)$, where $\widetilde{\xvet}_\ell=\psi(\widehat{\xvet}_\ell)$ and $\ell= 1, \dots, L$, is a sample drawn from the (cross-temporal) reconciled probability measure $\widetilde{\nu}$ defined in \eqref{eq:pfr}.
\end{theorem}
\begin{proof}
	See Theorem 4.5 from \cite{panagiotelis2023} using Definition \ref{def:pfr}.
\end{proof}
Theorem \ref{thm:rs} is the cross-temporal extension of Theorem 4.5 in \cite{panagiotelis2023}. It means that a sample from the reconciled distribution can be obtained by reconciling each member of a sample from the incoherent distribution. With this result, we can separate the mechanism used to generate the base forecasts samples from the reconciliation phase.


\subsection{Parametric framework: Gaussian reconciliation}\label{ssec:prob_pf}

It is possible to obtain a reconciled probabilistic forecast analytically for some parametric distributions, such as the multivariate normal \citep{corani2021, eckert2021, panagiotelis2023, wickramasuriya2021b}. In the cross-sectional framework, \cite{panagiotelis2023} show that, starting from an elliptical distribution for the base forecasts, the reconciled forecast distribution is also elliptical. Using the results shown in \autoref{sec:not}, we extend\footnote{We assume $H =1$ and simplify the notation by removing the $h$ suffix without loss of generality} this results to the cross-temporal case. To obtain a reconciled forecast using the multivariate normal distribution, we start with a base forecast distributed as $\mathcal{N}(\widehat{\xvet}, \Sigmavet)$, where $\widehat{\xvet}$ is the mean vector and $\Sigmavet$ is the covariance matrix of the base forecasts. The reconciled forecast distribution is then given by $\mathcal{N}(\widetilde{\xvet}, \widetilde{\Omegavet})$, where
\begin{equation}\label{eq:meanvar}
	\widetilde{\xvet} = \Mvet\widehat{\xvet} \quad \mbox{and} \quad \widetilde{\Omegavet} = \Mvet \Sigmavet \Mvet',
\end{equation}
where $\Mvet$ is the projection matrix defined in \eqref{eq:Mvet}.
Note that if we assume that $\Sigmavet = \Omegavet_{ct}$, then the covariance matrix in \eqref{eq:meanvar} simplifies to $\widetilde{\Omegavet} = \Mvet \Omegavet_{ct}$. In the cross-temporal case, sensibly estimating the covariance matrix $\Sigmavet$ can be difficult because we need to simultaneously consider both the temporal and cross-sectional structures. This requires many parameters to be estimated, which can be challenging in practice. Additionally, naively using one-step residuals to estimate the cross-temporal correlation structure can lead to an inappropriate estimate of the covariance matrix\footnote{In particular, some temporal covariances are fixed to zero (see Appendix C for more details).}. These challenges will be explored in more depth in the following sections.

\input{fig/gauss_sim.tex}

Focusing on the computational aspect\footnote{We use two R packages to sample from a the base forecast gaussian distribution: \texttt{MASS} \citep{mass2002} and \texttt{Rfast} \citep{rfast2022} in Sections \ref{sec:ausgdp} and \ref{sec:vn525}, respectively.}, we can take several steps to reduce the time required to obtain simulations from the reconciled forecast distribution. For example when dealing with a genuine hierarchical structure, it is not necessary to simulate from a normal distribution with a defined covariance matrix for the entire structure. Instead, we can utilize the properties of elliptical distributions to simulate from the high frequency bottom time series and then obtain the complete simulation through the $\Svet_{ct}$ matrix. Furthermore, we do not need to calculate the reconciled mean and variance and generate a new sample if we already have a sample from the normal distribution of the base forecasts; we can simply apply the point forecast reconciliation \eqref{eq:Mvet} as outlined in Theorem \ref{thm:rs}. The relationships between base and reconciled forecast distributions and their respective simulations through Theorem \ref{thm:rs} are depicted in \autoref{fig:gaussrel}.

\subsection{Non-parametric framework: bootstrap reconciliation}\label{ssec:boot}

Analytical expressions for the base and reconciled forecast distributions are sometimes challenging to obtain. Furthermore parametric assumptions can be restrictive and unrealistic. We propose a procedure called \textit{cross-temporal joint (block) bootstrap} (\textbf{ctjb}) to generate samples from the base forecast distributions that preserve cross-temporal relationships. This approach involves drawing samples of all series simultaneously from the most temporally aggregated level, and using the most temporally aggregated level to determine the corresponding time indices for the other levels.

Let $\widehat{\Evet}^{[k]}$ be the ($n \times N_k$) matrix of the residuals for $k \in \mathcal{K}$. \autoref{fig:res_boot} (on the left) provides a visualization of these matrices and how they are related to each other for the example in \autoref{fig:hierS}. It is assumed that the residuals cover four years ($N=4$): the green color corresponds to the first year, the blue to the second year, and so on. Further, let $\mathcal{M}_i$ be the model used to calculate the base forecasts and residuals for the $i^{th}$ series. %In this work, we assume $\mathcal{M}_i$ to be a univariate model, however nothing prevents the use of multivariate models, perhaps for different temporal levels or for groups of time series.
Assuming $H = 1$, $\tau$ is a random draw with replacement from $1,\dots, N$ and the $\ell^{th}$ bootstrap incoherent sample is
$\widehat{\xvet}_{i,\ell}^{[k]} = f_i(\mathcal{M}_i, \widehat{\evet}_{i}^{[k]})$,
where $f_i(\cdot)$ depends on the fitted %univariate 
model $\mathcal{M}_i$. That is, $\widehat{\xvet}_{i,l}^{[k]}$ is a sample path simulated for the $i^{th}$ series with error approximated by the corresponding block bootstrapped sample residual $\widehat{\evet}_{i}^{[k]}$, the $i^{th}$ row of
$$
	\widehat{\Evet}^{[k]}_{\tau} = \begin{bmatrix}
		\widehat{e}^{[k]}_{1,M_k(\tau-1)+1} & \dots  & \widehat{e}^{[k]}_{1,M_k\tau}   \\[-0.25cm]
		\vdots                              & \ddots & \vdots                          \\[-0.25cm]
		\widehat{e}^{[k]}_{n,M_k(\tau-1)+1} & \dots  & \widehat{e}^{[k]}_{n,M_k\tau} \
	\end{bmatrix}\qquad k \in \mathcal{K}.
$$
\autoref{fig:res_boot} (on the right) shows $\widehat{\Evet}^{[k]}_{\tau}$ for the quarterly cross-temporal hierarchy in \autoref{fig:hierS}.

\input{fig/bootstrap_fig.tex}

One of the main advantages of the cross-temporal joint bootstrap is that it allows us to accurately account for the dependence between the different levels of temporal aggregation and not only the cross-sectional dependencies. By sampling residuals from the most temporally aggregated level and using it to determine the indices for the other levels, we can ensure that the bootstrap sample reflects the underlying data distribution. Additionally, the cross-temporal joint bootstrap is easy to implement %in R \citep{rcoreteam2022} using the package \texttt{forecast} \citep{Rforecast} 
for many forecasting models, making it a practical and efficient tool. Furthermore, this approach is easily scalable in order to utilize multiple computing power simultaneously for each individual series. This can be especially useful when dealing with large datasets or when trying to speed up the analysis process.

\section{Cross-temporal covariance matrix estimation}\label{sec:shrtech}

As the covariance matrix $\Omegavet$ is unknown in practice, a natural estimate is the empirical sample covariance matrix of the base forecasts $\widehat{\Omegavet}$. In this section, our focus will be exclusively on the cross-temporal framework., this means that we have to estimate $r = n(k^\ast+m)[n(k^\ast+m)-1]/2$ different parameters. A possible solution to estimating many parameters when we have fewer observations than $r$, is to construct a shrinkage estimator \citep{efron1975a,efron1975,efron1977}, using a convex combination of $\widehat{\Omegavet}$ and a diagonal target matrix $\widehat{\Omegavet}_D = \widehat{\Omegavet} \odot \Ivet_{n(k^\ast+m)}$, such that $\widehat{\Omegavet}_{G} = \lambda \widehat{\Omegavet}_D + (1-\lambda) \widehat{\Omegavet}$, where $\lambda \in [0,1]$ is the shrinkage intensity parameter that can be estimate using the unbiased estimator proposed by \cite{ledoit2004a} (see \citealp{schafer2005}). The linear combination involving these two matrices is referred to as \textit{Global shrinkage} (\textit{G}), where all off-diagonal elements are shrunk towards zero. $\widehat{\Omegavet}_{G}$ corresponds to the matrix used by the reconciliation approach oct$(shr)$ shown in Appendix A. However, shrinking all off-diagonal elements to zero, when we know that the covariance matrix has a cross-sectional and/or temporal structure, results in information loss. Therefore, we propose to estimate a smaller matrix, and to use the cross-sectional and/or temporal structure to obtain a better estimator for the covariance matrix of the entire system. Given that $\Svet_{ct} = \Svet_{cs} \otimes \Svet_{te}$, it is possible to express the actual covariance matrix in terms of three smaller matrices such that
\begin{equation}\label{eq:OmSct}
\begin{aligned}
	\Omegavet &= \Svet_{ct}\Omegavet_{\textit{hf-bts}}\Svet_{ct}' \\[-0.25cm]
	&= \left(\Ivet_n \otimes \Svet_{te}\right)\Omegavet_{\textit{hf}}\left(\Ivet_n \otimes \Svet_{te}\right)' \\[-0.25cm]
	&= \left(\Svet_{cs} \otimes \Ivet_{m+k^\ast}\right)\Omegavet_{bts}\left(\Svet_{cs} \otimes \Ivet_{m+k^\ast}\right)',
\end{aligned}
\end{equation}
where $\Omegavet_{\textit{hf-bts}}$ is the $(n_b m\times n_b m)$ covariance matrix for the bottom time series at temporal aggregation level $k = 1$ (highest frequency bottom time series), $\Omegavet_{\textit{hf}}$ is the $(nm\times nm)$ covariance matrix related to all the high frequency time series and $\Omegavet_{bts}$ is the $[n_b(k^\ast + m)\times n_b(k^\ast + m)]$ covariance matrix related to bottom time series at any temporal aggregation.

Therefore, we can apply the idea of “Stein-type shrinkage" \citep{efron1977} to $\Omegavet_{\textit{hf-bts}}$, $\Omegavet_{\textit{hf}}$ and $\Omegavet_{\textit{bts}}$ by using the corresponding empirical base forecasts residuals estimation. We obtain the following expressions (see Appendix B for details):
\begin{itemize}[nosep]
	\item \textit{High frequency Bottom time series shrinkage matrix} (HB): \\ 
	$\widehat{\Omegavet}_{HB} = \lambda \Svet_{ct}\widehat{\Omegavet}_{\textit{hf-bts}, D}\Svet_{ct}'+ (1-\lambda) \Svet_{ct}\widehat{\Omegavet}_{\textit{hf-bts}}\Svet_{ct}'$;
	\item \textit{High frequency shrinkage matrix} (H): \\ $\widehat{\Omegavet}_{H}  = \lambda (\Ivet_{n} \otimes \Svet_{te})\widehat{\Omegavet}_{hf, D}(\Ivet_{n} \otimes \Svet_{te})' + (1-\lambda) (\Ivet_{n} \otimes \Svet_{te})\widehat{\Omegavet}_{\textit{hf}}(\Ivet_{n} \otimes \Svet_{te})'$;
	\item \textit{Bottom time series shrinkage matrix} (B): \\$\widehat{\Omegavet}_{B} = \lambda \left(\Svet_{cs} \otimes \Ivet_{m+k^\ast}\right)\widehat{\Omegavet}_{bts, D}\left(\Svet_{cs} \otimes \Ivet_{m+k^\ast}\right)' +  (1-\lambda) \left(\Svet_{cs} \otimes \Ivet_{m+k^\ast}\right)\widehat{\Omegavet}_{bts}\left(\Svet_{cs} \otimes \Ivet_{m+k^\ast}\right)'$,
\end{itemize}
where $\widehat{\Omegavet}_{l, D} = \Ivet_{n_b m}\odot\widehat{\Omegavet}_{j}$, $l = \{\textit{hf-bts}, \;\textit{hf}, \;\textit{bts}\}$, and $\lambda$ is the shrinkage parameter.

\begin{table}[!t]
	\centering
	\begingroup
	\spacingset{1.1}
	\begin{tabular}{cccccc}
		\toprule
		\textbf{Method}            & \textbf{\# of different parameters}                             & \textbf{GDP} & \textbf{Tourism}\\
		\midrule
		\addlinespace[0.25cm]
		\textit{G}                          & $r = \displaystyle\frac{n(k^\ast+m)[n(k^\ast+m)-1]}{2}$         & $221\,445$          & $108\,052\,350$ \\
		\addlinespace[0.25cm]
		\textit{HB} & $r_{HB} = \displaystyle\frac{n_bm[n_bm-1]}{2}<r$       & \makecell{$30\,876$ \\[-0.1cm] {\footnotesize$(86\%)$}}           & \makecell{$6\,655\,776$ \\[-0.1cm] {\footnotesize$(94\%)$}}  \\
		\addlinespace[0.25cm]
		\textit{H}               & $r_{HB}<\displaystyle\frac{nm[nm-1]}{2}<r$ & \makecell{$72\,390$ \\[-0.1cm] {\footnotesize$(67\%)$}}           & \makecell{$19\,848\,150$ \\[-0.1cm] {\footnotesize$(82\%)$}}\\
		\addlinespace[0.25cm]
		\textit{B}              & $r_{HB}<\displaystyle\frac{n_b(k^\ast+m)[n_b(k^\ast+m)-1]}{2}<r$ & \makecell{$94\,395$ \\[-0.1cm] {\footnotesize$(57\%)$}}           & \makecell{$36\,231\,328$ \\[-0.1cm] {\footnotesize$(66\%)$}}\\
		\addlinespace[0.25cm]
		\bottomrule
	\end{tabular}
	\endgroup
	\caption{Number of different parameters that need to be estimated for %the Monte Carlo simulation (AR(2), see \autoref{sec:mcsim}), 
	the Australian GDP (see \autoref{sec:ausgdp}) and the Australian Tourism Demand (see \autoref{sec:vn525}) forecasting experiments. %: %the first one has $3$ time series (one upper and two bottom) with temporal aggregation $\mathcal{K} = \{2, 1\}$; 
	%the first one has $95$ quarterly ($m = 4$ and $k^\ast = 3$) time series ($62$ free and $33$ constraints, see \citealp{giro2022}); the second one has a total of 525 monthly ($m = 12$ and $k^\ast = 16$) time series ($304$ bottom and $221$ upper). 
	The percentage reductions in the number of parameters compared to the global approach are reported in parentheses.}
	\label{tab:num_param}
	%\vspace*{-0.75\baselineskip}
\end{table}

Another important aspect is the number of parameters to be estimated through the residuals of the base forecasts. In \autoref{tab:num_param} we report the number of different parameters %for the Monte Carlo simulation (AR2) and 
for the two forecasting experiment: Australian GDP (see \autoref{sec:ausgdp}) and Australian Tourism Demand (see \autoref{sec:vn525}). In addition, we also calculate the percentage reductions in the number of parameters compared to the global approach. %, that is: $\% \text{ reduction} = 100(1-r_i/r_G)$ with $i \in \{\textit{HB}, H, B\}$. 
As we can see, \textit{G} involves a considerably large number of parameters compared to other estimators. \textit{HB} leads to the largest decrease of around 85\%, whereas approaches \textit{H} and \textit{B}  lie somewhere between \textit{G} and \textit{HB}. In general, as $m$ and $n$ increase (see Appendix B), using \textit{H} requires the estimation of less parameters than \textit{B}.

In the forecasting experiments that follow and in the simulation in Appendix C, we closely analyze these different constructions with a dual purpose. In particular, we use the full covariance matrix ($\lambda = 0$) of the base forecasts to obtain base forecast samples of the linearly constrained time series under Gaussianity. We also use the shrinkage versions as approximations of the covariance matrix to be used for reconciliation. This will allow us to better understand the properties and abilities of each parameterization.


\subsection{Multi-step residuals} \label{ssec:multi_res}

Model residuals may be used to estimate the covariance matrix in cross-temporal forecast reconciliation. In time series analysis, it is common to use residuals corresponding to one-step ahead forecasts. However, due to the temporal dimension in our setting, residuals corresponding to different forecast horizons are required. Thus, we define \textit{multi-step residuals} as $e_{i,h,j}^{[k]} = x_{i,j+h}^{[k]} - \widehat{x}_{i,j+h|j}^{[k]}$, where $i = 1,\dots,n$, $j = 1,\dots,N_k$ and $\widehat{x}_{i,j+h|t}^{[k]}$ is the $h$-step fitted value, calculated as the $h$-step-ahead forecast using data up to time $j$. In general, these residuals will be autocorrelated except when $h=1$.

Following \cite{difonzo2023}, we use a matrix organization of the residuals similar to the one for the base forecasts in \autoref{ssec:oct}. Specifically, let $N$ be the total number of observations for the most temporally aggregate time series. Then, the $N_k$-vectors of multi-step residuals for the temporal aggregation $k$ and the series $i$, $\evet_{i,h}^{[k]} =  \Big[e_{i,h,1}^{[k]} \quad e_{i,h,2}^{[k]} \quad \dots \quad e_{i,h,N_k}^{[k]}\Big]'$ with $h = 1,\dots, M_k$, can be organized in matrix form as
$$
	\Evet_i^{[k]} = \begin{bmatrix}
		e_{i,1,1}^{[k]}                     & e_{i,2,2}^{[k]}                     & \dots & e_{i,M_k,M_k}^{[k]} \\[-0.25cm]
		\vdots                            & \vdots                            &       & \vdots                  \\[-0.25cm]
		e_{i,1,N_k - M_k + 1}^{[k]} & e_{i,2,N_k - M_k + 2}^{[k]} & \dots & e_{i,M_k,N_k}^{[k]}         \\
	\end{bmatrix}.
$$
Let $\Evet_i = \Big[\Evet_i^{[m]} \quad \Evet_i^{[k_p-1]} \quad \dots \quad \Evet_i^{[1]}  \Big]$. Then the $[N \times n(m+k^\ast)]$ cross-temporal residual matrix is given by $
	\Evet = \Big[\Evet_1 \quad \Evet_2 \quad \dots \quad \Evet_n \Big]$.

To better understand the properties of the proposed alternatives, a simulation study was performed, and the results are shown in Appendix C. We have studied the effect of combining cross-sectional and temporal aggregations using a simple hierarchy, where the small size and nature of the data generating process make it possible to exactly calculate the true cross-temporal covariance structure, thus providing insights into the nature of the time series data involved in the forecast reconciliation process. We find that simulating base forecasts from multi-step residuals allows for a more accurate estimation of the covariance matrix and that reconciliation further improves the forecast accuracy. %of these estimates: accurate base forecasts for $k = 1$ assist the good performance for bottom-up and optimal cross-temporal reconciliation approaches, such as oct(wlsv) and oct(bdshr), which perform well in terms of both CRPS and ES (for details, see Appendix C).

\subsection{Overlapping residuals}\label{ssec:over_res}

Another issue that arises in the case of cross-temporal reconciliation is the low number of available residuals, especially for the higher orders of temporal aggregation. A possible solution is to use residuals calculated using overlapping series by allowing the year to have a varying starting time. To better explain how to calculate overlapping residuals, assume we have a single series $\yvet = [y_1 \; y_2 \; y_3 \; \dots\; y_{T-1}\; y_{T}]'$. We can construct $k$ non overlapping series such that $\xvet^{[k], s} = \left\{x^{[k],s}_{j}\right\}_{j = 1}^{N_k-s}$ where $x^{[k],s}_{j} = \displaystyle\sum_{t = (j-1)k+s+1}^{jk-s} y_t$, with $s = 0, \dots, (k-1)$. For example, suppose we have a biannual series with $k = 2$ and $T = 6$, then we can construct two annual time series depending on which time is deemed the start of the year: $\xvet^{[2], 0} =  \Big[x_1^{[2], 0}\quad x_2^{[2], 0}\quad x_{3}^{[2], 0} \Big]' =\Big[y_1 + y_2\quad y_3 + y_4\quad y_5 + y_6\Big]'$ and $\xvet^{[2], 1} = \Big[x_1^{[2], 1}\quad x_2^{[2], 1} \Big]' = \Big[y_2 + y_3\quad  y_4 + y_5 \Big]'$. To calculate overlapping residuals, we propose the following steps:
\begin{enumerate}[nosep]
	\item Fit a model to $\xvet^{[k], 0}$ (i.e., select an appropriate model and estimate the model parameters using the available data) and calculate the residuals.
	\item Apply the same model in step 1 to $\xvet^{[k], s}$ for $s = 1, \dots, k-1$, without re-estimating the parameters, and calculate the residuals.
\end{enumerate}

The resulting residuals can be used to estimate the covariance matrix in cross-temporal forecast reconciliation. This increases the number of available residuals, particularly when working with higher frequency observations such as monthly or daily data.
It is important to note that this approach assumes that the model used in step 1 is appropriate for all the different series $\xvet^{[k], s}$. Some seasonal models will not be appropriate as the seasonal pattern will be shifted for different values of $s$. However, this will not affect seasonal ARIMA models as the seasonality is defined in terms of lags which are unaffected by the value of $s$.

\section{Forecasting Australian GDP}\label{sec:ausgdp}

The Australian Quarterly National Accounts (QNA) dataset has been widely studied in the literature on forecast reconciliation \citep{athanasopoulos2020, difonzo2023}. Building on these results (see Appendix D.1), we now consider cross-temporally reconciled probabilistic forecasts.

We use univariate ARIMA models\footnote{We use the \texttt{auto.arima} function from the R package \texttt{forecast} \citep{Rforecast}.} to obtain quarterly base forecasts for the $n = 95$ QNA time series, spanning the period 1984:Q4 -- 2018:Q1, defining GDP from both the Income and Expenditure sides. We perform a rolling forecast experiment with an expanding window: the first training sample spans the period 1984:Q4 to 1994:Q3, and the last ends in 2017:Q1, for a total of 91 forecast origins. For the temporal aggregation dimension we aggregate the quarterly data to both semi-annual and annual. We obtain $4$-step, $2$-step and $1$-step ahead base forecasts respectively from the quarterly, semi-annual and annual frequencies, i.e., $\mathcal{K} = \{4,2,1\}$.


\begin{table}[!t]
	\centering
	%{\small
	\begin{tabular}{M{0.17\linewidth}|L{0.77\linewidth}}
		\toprule
		\textbf{Label} & \textbf{Description} \\
		\midrule
		ct$(bu)$ & Simple cross-temporal bottom-up (\autoref{ssec:ctbu}). \\
		\addlinespace[0.15cm]
		ct$(\;\cdot\;, bu_{te})$ & Partly bottom-up (\autoref{ssec:ctbu}) starting from cross-sectional reconciled forecasts using the $shr$ and $wls$ approaches (see Appendix A).\\
		\addlinespace[0.15cm]
		ct$(wlsv_{te}, bu_{cs})$ & Partly bottom-up (\autoref{ssec:ctbu}) starting from temporally reconciled forecasts using the $wlsv$ approach (see Appendix A).\\
		\addlinespace[0.15cm]
		oct$(\;\cdot\;)$ & Optimal cross-temporal reconciliation for the $ols$, $struc$, $wlsv$ and $bdshr$ approaches (see Appendix A). One-step residuals were used with $wlsv$ and $bdshr$. \\
		\addlinespace[0.15cm]
		oct$_h(\;\cdot\;)$ & Optimal cross-temporal reconciliation with multi-step residuals (see \autoref{ssec:multi_res}) for the approaches presented in \autoref{sec:shrtech}: $shr$ stands for \textit{Global shrinkage}, $hshr$ for \textit{High frequency shrinkage}, $bshr$ for \textit{bottom time series shrinkage}, $hbshr$ for \textit{High frequency bottom time series shrinkage}.\\
		\addlinespace[0.15cm]
		oct$_o(\;\cdot\;)$ & Optimal cross-temporal reconciliation with overlapping residuals (see \autoref{ssec:over_res}) for the $wlsv$ and $bdshr$ approaches (see Appendix A). \\
		\addlinespace[0.15cm]
		oct$_{oh}(\;\cdot\;)$ & Optimal cross-temporal reconciliation with overlapping and multi-step residuals (see Section \ref{ssec:multi_res} and \ref{ssec:over_res}) for the approaches presented in \autoref{sec:shrtech}: $shr$ stands for \textit{Global shrinkage}, $hshr$ for \textit{High frequency shrinkage}.\\
		\bottomrule
	\end{tabular}%}
	\caption{Cross-temporal reconciliation approaches for %the Monte Carlo simulation (see \autoref{sec:mcsim}), 
	the Australian GDP (see \autoref{sec:ausgdp}) and the Australian Tourism Demand (see \autoref{sec:vn525}) forecasting experiments. All the reconciliation procedures are available in \texttt{FoReco} \citep{foreco2023}.}
	\label{tab:notation}
	%\vspace*{-0.75\baselineskip}
\end{table}

The base forecast samples in the Gaussian case are obtained using the sample covariance matrices with the \textit{Global} (G) and \textit{High frequency} (H) parameterization (\autoref{sec:shrtech}), since it is not possible to identify a unique representation for the other cases\footnote{When simultaneously considering Income and Expenditure sides hierarchies, the result is a general linearly constrained time series, where bottom and upper time series are not uniquely defined, making unfeasible the cross-sectional bottom-up reconciliation approach \citep{giro2022}.}. We compare the results obtained using multi-step residuals with and without overlapping, in order to measure the benefit of obtaining overlapping residuals. In the non-parametric case, we use the cross-temporal joint bootstrap (ctjb) presented in \autoref{ssec:boot}. Finally, to reconcile the resulting (1000) base forecasts samples, we have applied the following techniques\footnote{In Appendix D.2, we show the results with shrunk covariance matrices. We also report the results obtained using one-step residuals in the reconciliation.} (see \autoref{tab:notation}): ct$(shr_{cs}, bu_{te})$, ct$(wls_{cs}, bu_{te})$, oct$_o(wlsv)$, oct$_o(bdshr)$, oct$_{oh}(shr)$ and oct$_{oh}(hshr)$.

The accuracy of the probabilistic forecasts is evaluated using the Continuous Ranked Probability Score (CRPS, \citealp{gneiting2014}), %given by
%\begin{equation}\label{eq:crps}
%	\operatorname{CRPS}(\widehat{P}_i, z_i)=\frac{1}{L} \sum_{l=1}^{L}\left|x_{i,l}-z_i\right|-\frac{1}{2 L^{2}} \sum_{l=1}^{L} \sum_{j=1}^{L}\left|x_{i,l}-x_{i,j}\right|, \quad i = 1,\dots,n,
%\end{equation}
%where $\widehat{P}_i(\omega)=\displaystyle\frac{1}{L} \sum_{l=1}^{L} \mathbf{1}\left(x_{i,l} \leq \omega\right)$, $\xvet_{1}, \xvet_{2}, \dots, \xvet_{L}\in \mathbb{R}^{n}$ is a collection of $L$ random draws from the predictive distribution and $\zvet \in \mathbb{R}^{n}$ is the observation vector. CRPS 
which is an index that considers the single series and provides us a marginal evaluation of the approaches. In addition, we employ the Energy Score (ES, \citealp{gneiting2014}), that is the CRPS extension to the multivariate case, to evaluate the forecasting accuracy for the whole system \citep{panagiotelis2023, wickramasuriya2021b}.
%\begin{equation}\label{eq:es}
%	\operatorname{ES}(\widehat{P}, \zvet)=\frac{1}{L} \sum_{l=1}^{L}\left\|\xvet_{l}-\zvet\right\|_{2}-\frac{1}{2(L-1)} \sum_{i=1}^{L-1}\left\|\xvet_{l}-\xvet_{l+1}\right\|_{2}
%\end{equation}
%where $	\lVert \cdot \rVert_2$ is the L$_2$ norm. 
In particular, we consider the geometric mean of the relative CRPS \citep{fleming1986}, and the relative ES:
\begin{equation}\label{eq:skill}
	\operatorname{\overline{RelCRPS}}_{j,s}^{[k]} = \left(\prod_{i = 1}^n \frac{CRPS^{[k]}_{i, j, s}}{CRPS^{[k]}_{i, 0, 0}}\right)^{\frac{1}{n}} \qquad \mathrm{and} \qquad \operatorname{RelES}_{j,s}^{[k]} = \frac{ES^{[k]}_{j, s}}{ES^{[k]}_{0, 0}},
\end{equation}
where $j$ denotes the reconciliation approach and $s$ indicates the approach used to simulate the base forecasts. As a reference approach ($s=0$ and $j=0$), we consider the base forecasts produce by the Bootstrap approach. If we consider all the temporal aggregation orders (i.e. $\forall k \in \mathcal{K}$), the overall accuracy indices are given by, respectively, 
\begin{equation}\label{eq:skill_all}
	\operatorname{\overline{RelCRPS}}_{j,s} = \left(\prod_{\substack{i = 1, \dots, n \\ k \in \mathcal{K}}}\frac{CRPS^{[k]}_{i, j, s}}{CRPS^{[k]}_{i, 0, 0}}\right)^{\frac{1}{n(k^\ast+m)}}\mbox{and } \operatorname{\overline{RelES}}_{j,s}= \left(\prod_{k \in \mathcal{K}}\frac{ES^{[k]}_{j, s}}{ES^{[k]}_{0, 0}}\right)^{\frac{1}{(k^\ast+m)}}.
\end{equation}
%and
%\begin{equation}\label{eq:skillES_all}
%	\operatorname{AvgRelES}_{j,s}= \left(\prod_{k \in \mathcal{K}}\frac{ES^{[k]}_{j, s}}{ES^{[k]}_{0, 0}}\right)^{\frac{1}{(k^\ast+m)}}.
%\end{equation}

\subsection{Results}\label{ssec:ausresults}

\begin{table}[!p]
	\centering
	\begingroup
	\spacingset{1}
	\fontsize{9}{11}\selectfont
	\input{tab/AusGDP/sam_crps.tex}
	\endgroup
	\caption{$\overline{RelCRPS}$ defined in \eqref{eq:skill} and \eqref{eq:skill_all} for the Australian QNA dataset. Approaches performing worse than the benchmark (bootstrap base forecasts, ctjb) are highlighted in red, the best for each column is marked in bold, and the overall lowest value is highlighted in blue. The reconciliation approaches are described in \autoref{tab:notation}.}
	\label{tab:auscrps}
%	\vspace*{-0.75\baselineskip}
%\end{table}
%\begin{table}[!t]
	\centering
	\begingroup
	\spacingset{1}
	\fontsize{9}{11}\selectfont
	\input{tab/AusGDP/sam_es.tex}
	\endgroup
	\caption{ES ratio indices defined in \eqref{eq:skill} and \eqref{eq:skill_all} for the Australian QNA dataset. Approaches performing worse than the benchmark (bootstrap base forecasts, ctjb) are highlighted in red, the best for each column is marked in bold, and the overall lowest value is highlighted in blue. The reconciliation approaches are described in \autoref{tab:notation}.}
	\label{tab:auses}
	%\vspace*{-0.75\baselineskip}
\end{table}

Forecasting accuracy indices based on CRPS and ES are presented in Tables \ref{tab:auscrps} and \ref{tab:auses}, respectively. As a benchmark approach, we use the base forecasts calculated using the bootstrap method. For base forecasts, we find that using a parametric approach with the normal distribution performs better than the non-parametric bootstrap approach. This is likely due to the limited number of residuals available for bootstrapping, which does not allow for sufficient exploration of the data. Directly specifying diagonal covariance matrices seems to be more effective than shrinking to a target covariance matrix. Among all the procedures, ct$(wls_{cs},bu_{te})$ and oct$_o(wlsv)$ show the greatest relative gains. In contrast, oct$_{oh}(shr)$ and $oct_{oh}(hshr)$ do not show much improvement. Furthermore, the greatest improvements are observed for higher temporal aggregation levels.


\begin{figure}[p]
	\centering
	\includegraphics[width = 0.45\linewidth]{fig/AusGDP/hsamoh.pdf}
	\includegraphics[width = 0.45\linewidth]{fig/AusGDP/ctjb.pdf}
	\caption{MCB Nemenyi test for the Australian QNA dataset using CRPS at different temporal aggregation levels for the Gaussian (using overlapping and multi-step residuals, H) and the non-parametric bootstrap approaches. In each panel, the Friedman test p-value is reported in the lower right corner. The mean rank of each approach is shown to the right of its name. Statistical differences in performance are indicated if the intervals of two forecast reconciliation procedures do not overlap. Thus, approaches that do not overlap with the blue interval are considered significantly worse than the best, and vice-versa.}
	\label{fig:mcb}
\end{figure}

We utilize the non-parametric Friedman test and the post hoc “Multiple Comparison with the Best" (MCB) Nemenyi test \citep{koning2005, kourentzes2019, makridakis2022} to determine if the forecasting performances of the different techniques are significantly different from one another.
\autoref{fig:mcb} presents the MCB using the CRPS. The probabilistic forecasts from ct$(wls_{cs},bu_{te})$ and oct$_o(wlsv)$ are significantly better than the base forecasts at any level of aggregation.

Overall, we find that using overlapping residuals almost always leads to a greater improvement in terms of both ES and CRPS. %, and this is also generally true for reconciliation. 
Forecasts at the most aggregated level (year) seem to benefit the most from reconciliation, and using one-step overlapping residuals appears to be sufficient to improve forecasts if the generation of the base forecasts sample paths takes into account the multi-step structure.



\section{Forecasting Australian Tourism Demand}\label{sec:vn525}

The Australian Tourism Demand dataset \citep{wickramasuriya2019} measures the number of nights Australians spent away from home. It includes 228 monthly observations of Visitor Nights (VN) from January 1998 to December 2016, and has a cross-sectional grouped structure based on a geographic hierarchy crossed by purpose of travel. The geographic hierarchy comprises seven states, 27 zones, and 76 regions, for a total of 111 nested geographic divisions. Six of these zones (see Table E.14 in Appendix E) are each formed by a single region, resulting in a total of 105 unique nodes in the hierarchy. The purpose of travel comprises four categories: holiday, visiting friends and relatives, business, and other.
To avoid redundancies \citep{difonzo2022a}, 24 nodes are not considered, resulting in an unbalanced hierarchy of 525 unique nodes instead of the theoretical 555 with duplicated nodes.
The dataset includes the 304 bottom series, which are aggregated into 221 upper time series. \autoref{tab:nseries} omits duplicated entries and updates the information in Table 7 from \cite{wickramasuriya2019}. This data can be temporally aggregated into 2, 3, 4, 6, or 12 months ($\mathcal{K} = \{12,4,3,2,1\}$).

\begin{table}[!t]
	\spacingset{1.1}
	\setlength{\tabcolsep}{10pt}
	\centering
	\begin{tabular}{c|cc|c}
		\toprule
		& \multicolumn{3}{c}{\textbf{Number of series}}\\
		& \textbf{GD} & \textbf{PT} & \textbf{Tot}. \\
		\midrule
		Australia & 1 & 4 & 5 \\
		States & 7 & 28 & 35 \\
		Zones$^*$ & 21 & 84 & 105 \\
		Regions & 76 & 304 & 380 \\
		\bottomrule
		\textbf{Total} & \textbf{105}                                  & \textbf{420}   & \textbf{525} \\
		\bottomrule
		\addlinespace[0.3em]
	\end{tabular}\\
	{\footnotesize \textbf{*} 6 Zones with only one Region are included in Regions. GD: Geographic Division; PT: Purpose of Travel.}\\[0.1cm]
	\caption{\label{tab:nseries} Grouped time series for the Australian Tourism Demand dataset. }
	%\vspace*{-0.75\baselineskip}
\end{table}

We perform a rolling forecast experiment with an expanding window. The process begins by using the first 10 years, from January 1998 to December 2008, to generate forecasts for the entire following year (2009). Then, the training set is increased by one month. This process is repeated until the last training set is used (January 1998 to December 2015) with a total of 85 different test sets. For the temporal aggregation dimension we aggregate the monthly data up to annual data. We obtain $12$-step, $6$-step, $4$-step, $3$-step, $2$-step and $1$-step ahead base forecasts respectively from the monthly data and the aggregation over 2, 3, 4, 6, and 12 months. ETS models selected by minimizing the AICc criterion \citep{Rforecast} %with the R package \texttt{forecast} 
are fitted to the log-transformed data, with the resulting base forecasts being back-transformed to produce non-negative forecasts \citep{wickramasuriya2020}.

The (1000) base forecast samples are obtained using the Gaussian approach with sample\footnote{The results with shrunk covariance matrices are available in Appendix E.2.} covariance matrices (\autoref{sec:shrtech}) using multi-step residuals\footnote{We do not include overlapping, as we are unable to correctly determine the residuals for the overlapping series using ETS models (see \autoref{ssec:over_res}).} and the bootstrap approach (\autoref{ssec:boot}). For reconciliation, 11 different approaches have been adopted (see \autoref{tab:notation}): ct$(bu)$, ct$(shr_{cs}, bu_{te})$, ct$(wlsv_{te}, bu_{cs})$, ct$(ols)$, oct$(struc)$, oct$(wlsv)$, oct$(bdshr)$, oct$_h(hbshr)$, oct$_h(bshr)$, oct$_h(hshr)$, and oct$_h(shr)$.

Negative forecasts may be produced during the reconciliation phase \citep{wickramasuriya2020, difonzo2022a, difonzo2023a} thus generating unreasonable values (e.g., a negative forecast for tourism demand makes no sense). To overcome this limitation (see Appendix E.1), we applied the simple heuristic proposed by \cite{difonzo2022b, difonzo2023a}. Following Theorem \ref{thm:rs}, we are thus able to obtain reconciled samples respecting non-negativity constraints starting from an incoherent sample simulated from a Gaussian distribution. Finally, to evaluate the performance, we employ the Continuous Ranked Probability Score (CRPS), the Energy Score (ES), and the “Multiple Comparison with the Best" (MCB) Nemenyi test, introduced in Sections \ref{sec:ausgdp} and \ref{ssec:ausresults}. 

\subsection{Results}

The CRPS and ES indices are shown, respectively, in Tables \ref{tab:vncrps} and \ref{tab:vnes} for monthly, quarterly and annual forecasts\footnote{The complete results for all temporal aggregation levels are reported in Appendix E.2.}. These tables are divided by different temporal levels and each column uses a different approach to calculate the base forecasts, referred to as “base". The bootstrap method is used as a benchmark to calculate the accuracy indices.

\begin{table}[!tb]
	\centering
	\begingroup
	\spacingset{1}
	\fontsize{9}{10}\selectfont
	\input{tab/VN525/sam_crps_part.tex}
	\endgroup
	\caption{$\overline{RelCRPS}$ defined in \eqref{eq:skill} and \eqref{eq:skill_all} for the Australian Tourism Demand dataset. %A lower value, indicates a more accurate forecast. 
	Approaches performing worse than the benchmark (bootstrap base forecasts, ctjb) are highlighted in red, the best for each column is marked in bold, and the overall lowest value is highlighted in blue. The reconciliation approaches are described in \autoref{tab:notation}.}
	\label{tab:vncrps}
	%\vspace*{-0.75\baselineskip}
\end{table}

\begin{table}[!tb]
	\centering
	\begingroup
	\spacingset{1}
	\fontsize{9}{10}\selectfont
	\input{tab/VN525/sam_es_part.tex}
	\endgroup
	\caption{ES ratio indices defined in \eqref{eq:skill} and \eqref{eq:skill_all} for the Australian Tourism Demand dataset. %A lower value, indicates a more accurate forecast. 
	Approaches performing worse than the benchmark (bootstrap base forecasts, ctjb) are highlighted in red, the best for each column is marked in bold, and the overall lowest value is highlighted in blue. The reconciliation approaches are described in \autoref{tab:notation}.}
	\label{tab:vnes}
	\vspace*{-0.35\baselineskip}
\end{table}

Base forecasts using a Gaussian approach are better in terms of both CRPS and ES compared to the bootstrap approach (the benchmark). Assumptions of truncated Gaussianity (Gaussian with negative values set to zero) may seem strict, but given the limited number of residuals, it can lead to improved forecasts in terms of CRPS and ES. Bootstrap forecasts suffer from the limited number of available residuals, leading in general to lower forecast accuracy. The Gaussian approach overcomes this limitation and provides better results. Regarding the different covariance matrix estimates for Gaussian base forecasts, there are no big differences. For this reason, using only the high frequency bottom time series can be useful to estimate fewer parameters and reduce the initial high dimensionality.

In the Gaussian case, bottom-up ct$(bu)$ and partly bottom-up techniques like ct$(shr_{cs}, bu_{te})$ and ct$(wlsv_{te}, bu_{cs})$ lead to better results than the benchmark (bootstrap base forecasts). However, it's not always guaranteed that the improvement is higher than the starting base forecasts (by comparing the value of each column). This is particularly true for higher levels of temporal aggregation (see Appendix E.2 for details). Overall, oct$(bdshr)$ in terms of CRPS is almost always the best. The shrinkage approach oct$_h(hshr)$ performs well in the bootstrap case: it is competitive with oct$(bdshr)$ at lower temporal frequency ($k \in \{2,1\}$) and it is able to improve for $k\ge 3$. In terms of ES, oct$(bdshr)$ is still competitive, although it does not always show the best relative performance. In this case, approaches that attempt to explicitly take into account temporal and cross-sectional relationships, such as oct$_h(hbshr)$ and oct$_h(bshr)$, perform better. It is also worth noting that techniques that don't make use of residuals like oct$(ols)$ and oct$(struc)$ prove to be competitive by consistently improving on the base forecasts in terms of both CRPS and ES.

\begin{figure}[p]
	\centering
	\includegraphics[width = 0.45\linewidth]{fig/VN525/ctjb_part.pdf}
	\includegraphics[width = 0.45\linewidth]{fig/VN525/hbsamh_part.pdf}
	\caption{MCB Nemenyi test for the Australian Tourism Demand dataset using CRPS at different temporal aggregation levels for the Gaussian (multi-step residuals, HB) and the non-parametric bootstrap approaches. In each panel, the Friedman test p-value is reported in the lower right corner. The mean rank of each approach is shown to the right of its name. Statistical differences in performance are indicated if the intervals of two forecast reconciliation procedures do not overlap. Thus, approaches that do not overlap with the blue interval are considered significantly worse than the best, and vice-versa.}
	\label{fig:vnmcb}
\end{figure}

\autoref{fig:vnmcb} shows the MCB using the CRPS for the Gaussian approach using multi-step residuals (HB) and the non-parametric bootstrap approach. In general, the partly bottom-up procedure improves with respect to base forecasts at monthly level, but optimal cross-temporal procedures are always better. In the bootstrap framework, we can identify a group of three procedures, oct$(bdshr)$, oct$(hshr)$ and oct$(struc)$ that are almost always in the group of the best approaches (denoted by the blue dot). In the Gaussian framework, oct$(wlsv)$, oct$(struc)$, and oct$(bdshr)$ are always significantly better than base forecasts and equivalent in terms of results for temporal aggregation orders greater than 2. For monthly series, oct$(bdshr)$ is always significantly better than all other approaches.

\section{Conclusion}\label{sec:conclusion}

In this paper, we extend the probabilistic reconciliation setting developed by \cite{panagiotelis2023} for the cross-sectional case to the cross-temporal framework. Through appropriate notation, we show how theorems and definitions valid for the cross-sectional case can be reinterpreted and extended. The general notation proposed can help investigate extensions following different probabilistic approaches, such as those in \cite{jeon2019}, \cite{bentaieb2021} and \cite{corani2022}. We propose a Gaussian and a bootstrap approach to simulate the base forecasts able to take into account both cross-sectional and temporal relationships simultaneously, opening the way for further research into  cross-temporal probabilistic forecasting.

Moreover, we analyze the use of residuals, showing that one-step residuals fail to capture the temporal structure, and we propose multi-step residuals that can fully capture the cross-temporal relationships. In Appendix C, we present a simple simulation to investigate using the different types of residuals. When dealing with covariance matrices (due to the high-dimensionality of the cross-temporal setting), we propose four alternative forms to reduce the number of parameters to be estimated, showing that the overlapping residuals may reduce the high-dimensionality burden by increasing the number of available residuals. These ideas are worth requiring further investigation in future works.

Finally, we perform empirical applications on two datasets commonly used in forecast reconciliation research: Australian GDP from Income and Expenditure sides and Australian Tourism Demand. We find that in both cases optimal cross-temporal reconciliation approaches significantly improve on base forecasts. We also compare these with partly bottom-up techniques that use uni-dimensional reconciliations (either cross-sectional or temporal) and confirm that simultaneously exploiting both dimensions in reconciliation produces better results, especially at higher levels of temporal aggregation. In conclusion, reconciliation approaches can play an important role to improve the accuracy of forecasts in a probabilistic framework while achieving the important attribute of producing coherent forecasts.

\appendix
\setcounter{table}{0}
\renewcommand{\thetable}{\Alph{section}.\arabic{table}}

\if0\blind
{
  \phantomsection\addcontentsline{toc}{section}{Acknowledgments}
\section*{Acknowledgments}

\noindent Tommaso Di Fonzo and Daniele Girolimetto acknowledge financial support from project PRIN2017 “HiDEA: Advanced Econometrics for High-frequency Data”, 2017RSMPZZ. Rob Hyndman acknowledges the support of the Australian Government through the Australian Research Council Industrial Transformation Training Centre in Optimisation Technologies, Integrated Methodologies, and Applications (OPTIMA), Project ID IC200100009.
} \fi

\phantomsection\addcontentsline{toc}{section}{References}
\begingroup
\spacingset{1.6}
\setlength{\bibsep}{0pt plus 0.3ex}
\bibliographystyle{chicago}
\bibliography{mybibfile}
\endgroup
\end{document}
